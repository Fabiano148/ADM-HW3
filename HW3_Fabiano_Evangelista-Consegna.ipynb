{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fevangel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fevangel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from langdetect import detect\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the txt file that collects the list of the urls\n",
    "## First, I open a txt file where I'm going to write the list of the books' url.\n",
    "## With a for loop I check the first 300 pages (I stopped on page 12 due to lack of time) and, through BeautifulSoup, I extract every bookTitle, adding them to a list (\"list_ads\").\n",
    "## With a for loop on \"list_ads\", I add the string 'https://www.goodreads.com/' to every book, obtaining the relative url. I added every url to a list (\"list_of_url\").\n",
    "## With a for loop on the \"list_of_url\", I get every url and I save the relative html page in the directory of the relative page (for example, the html of the books in the first page are saved in the directory \"page_1\" with the name \"article_<num_article>\"). I create the relative directory if not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I open a txt file where I'm going to write the list of the books' url\n",
    "txt_url = open(\"List_of_url.txt\", \"w\")\n",
    "# With a for loop I check the first 300 pages and, through BeautifulSoup, I extract every bookTitle, adding them to a list (\"list_ads\")\n",
    "for num_page in range(1,301): #range(301)\n",
    "    list_of_url = []\n",
    "    best_books_ever = 'https://www.goodreads.com/list/show/1.Best_Books_Ever?page='+str(num_page)\n",
    "    page = urlopen(best_books_ever)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    list_ads = soup.findAll('a' ,class_='bookTitle')\n",
    "# With a for loop on \"list_ads\", I add the string 'https://www.goodreads.com/' to every book, obtaining the relative url. I added every url to a list (\"list_of_url\").\n",
    "    for i in list_ads:\n",
    "        list_of_url.append('https://www.goodreads.com/'+str(i.get('href')))\n",
    "#With a for loop on the \"list_of_url\", I get every url and I save the relative html page in the directory of the relative page (for example, the html of the books in the first page are saved in the directory \"page_1\" with the name \"article_<num_article>\"). I create the relative directory if not exists.\n",
    "    for line in range(len(list_of_url)):\n",
    "        txt_url.write(list_of_url[line]+'\\n')\n",
    "        html = urlopen(list_of_url[line]).read().decode('utf-8')\n",
    "        directory = \"page_\"+str(num_page)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        urllib.request.urlretrieve(list_of_url[line], directory+\"/article_\"+str((num_page-1)*100 + line)+\".html\")\n",
    "txt_url.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a tsv file for every html page\n",
    "## With a for loop on the pages (directory: \"page_1\", \"page_2\", etc) and a for loop on the html inside every page/directory (\"article_0.html, \"article_1.html\" and so on in \"page_1\" directory etc) I parse every html page through BeautifulSoup\n",
    "## For every html I verify that the Plot is in english, through \"detect\". If it is in english, I consider that book, otherwise I discard it.\n",
    "## I open a tsv file for every html in every page/directory and I write the header of the tsv file\n",
    "## I find every information in html page through BeautifulSoup, and I add this information to the relative tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a for loop on the pages (directory: \"page_1\", \"page_2\", etc) and a for loop on the html inside every page/directory (\"article_0, \"article_1\" and so on in \"page_1\" directory etc) I parse every html page through BeautifulSoup\n",
    "for page in range(1,12): #for page in (1:301):\n",
    "    for html in range(0,100):\n",
    "        try:\n",
    "            soup = BeautifulSoup(open(\"page_\"+str(page)+\"/article_\"+str((page-1)*100 + html)+\".html\",encoding=\"utf8\"), \"html.parser\")\n",
    "            # For every html I verify that the Plot is in english, through \"detect\". If it is in english, I consider that book, otherwise I discard it.\n",
    "            plot_verified = soup.find('div', id='descriptionContainer')\n",
    "            if plot_verified is not None:\n",
    "                Plot_verified = plot_verified.text.strip()\n",
    "            if detect(Plot_verified) == 'en':   \n",
    "                #I open a tsv file for every html in every page/directory and I write the header of the tsv file\n",
    "                with open('page_'+str(page)+'/article'+str((page-1)*100 + html)+'.tsv','wt', encoding=\"utf8\" ) as out_file:\n",
    "                    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                    tsv_writer.writerow(['bookTitle', 'bookSeries','bookAuthors','ratingValue','ratingCount','reviewCount','Plot','NumberofPages','Publishing_Date','Characters','Setting','Url'])\n",
    "                    # I find every information in html page through BeautifulSoup, and I add this information to the relative tsv file.\n",
    "                    title = soup.find('h1')\n",
    "                    if title is not None:\n",
    "                        bookTitle = title.text.strip()\n",
    "                    else:\n",
    "                        bookTitle = ''\n",
    "                    series = soup.find('h2', id='bookSeries')\n",
    "                    if series is not None:\n",
    "                        bookSeries = series.text.strip()\n",
    "                    else:\n",
    "                        bookSeries=''\n",
    "                    author = soup.find('div', id='bookAuthors')\n",
    "                    if author is not None:\n",
    "                        bookAuthors = author.text.strip()\n",
    "                    else:\n",
    "                        bookAuthors=''\n",
    "                    ratings = soup.find(\"span\", {\"itemprop\": \"ratingValue\"})\n",
    "                    if ratings is not None:\n",
    "                        ratingValue = ratings.text.strip()\n",
    "                    else:\n",
    "                        ratingValue=''\n",
    "                    giv_ratings = soup.find(\"meta\", {\"itemprop\": \"ratingCount\"})\n",
    "                    if giv_ratings is not None:\n",
    "                        ratingCount = giv_ratings.text.strip()\n",
    "                    else:\n",
    "                        ratingCount=''\n",
    "                    num_reviews = soup.find(\"meta\", {\"itemprop\": \"reviewCount\"})\n",
    "                    if num_reviews is not None:\n",
    "                        reviewCount = num_reviews.text.strip()\n",
    "                    else:\n",
    "                        reviewCount=''\n",
    "                    plot = soup.find('div', id='descriptionContainer')\n",
    "                    if plot is not None:\n",
    "                        Plot = plot.text.strip()\n",
    "                    else:\n",
    "                        Plot=''\n",
    "                    num_pages = soup.find(\"span\", {\"itemprop\": \"numberOfPages\"})\n",
    "                    if num_pages is not None:\n",
    "                        NumberofPages = num_pages.text.strip()\n",
    "                    else:\n",
    "                        NumberofPages=''\n",
    "                    published = soup.findAll('div', class_='row')\n",
    "                    try:\n",
    "                        pub = published[1]\n",
    "                        if pub is not None:\n",
    "                            Publishing_Date = pub.text.strip()\n",
    "                        else:\n",
    "                            Publishing_Date=''\n",
    "                    except:\n",
    "                        Publishing_Date=''\n",
    "                    characters = soup.findAll('div', class_='infoBoxRowItem')\n",
    "                    try:\n",
    "                        char = characters[4]\n",
    "                        if char is not None:\n",
    "                            Characters = char.text.strip()\n",
    "                        else:\n",
    "                            Characters=''\n",
    "                    except:\n",
    "                        Characters=''\n",
    "                    setting = soup.findAll('div', class_='infoBoxRowItem')\n",
    "                    try:\n",
    "                        sett = setting[5]\n",
    "                        if sett is not None:\n",
    "                            Setting = sett.text.strip()\n",
    "                        else:\n",
    "                            Setting=''\n",
    "                    except:\n",
    "                        Setting=''\n",
    "                    URL = soup.findAll('link')\n",
    "                    try:\n",
    "                        Url = URL[0].get('href')\n",
    "                    except:\n",
    "                        Url = ''\n",
    "                    tsv_writer.writerow([bookTitle, bookSeries,bookAuthors,ratingValue,ratingCount,reviewCount,Plot,NumberofPages,Publishing_Date,Characters,Setting,Url])\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stopwords, punctuation and stemming.\n",
    "# Creation of a json \"vocabulary\" that maps each word in the html pages to an integer \"term_id\".\n",
    "# Creation of a json \"inverted_index\", where for every \"term_id\" I write all the \"document_i\" in which the relative word is present.\n",
    "## I create the two vocabularies \"vocabulary\" and \"index_vocabulary\"\n",
    "## I use a for loop on the pages (directory: \"page_1\", \"page_2\", etc) and a for loop on the tsv file inside every page/directory (\"article0.tsv\", \"article1.tsv\" and so on in \"page_1\" directory etc)\n",
    "## I apply the stemming to the words in the Plot.\n",
    "## I tokenize the word in the \"Plot\" and I consider every english stopwords. With a for loop on the words extract with tokenizer, I check if the word isn't a stopword. If so, I add this word to the list \"filtered_sentence\".\n",
    "## I create a \"list_without_punct\" with the words of the \"Plot\" without punctuation.\n",
    "## With a for loop, I check if the words in \"filtered_sentence\" are also in \"list_without_punct\" (to discard punctuation). If so I add the word to filtered_sentence_without_punct.\n",
    "## I verify if a word is already present in the \"vocabulary\". If not, I add this word to the \"vocabulary\", associating it to a sequential integer (key). I also add that document to the \"inverted_index\" for that sequential integer (key).\n",
    "## If that document is not yet present in the \"inverted_index\" for that sequential integer (key), I add it.\n",
    "## I write the two dictionaries \"vocabulary\" and \"inverted_index\" on the relative json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create the two vocabularies \"vocabulary\" and \"index_vocabulary\"\n",
    "vocabulary = {}\n",
    "index_vocabulary = 1\n",
    "inverted_index = {}\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "porter = PorterStemmer()\n",
    "# I use a for loop on the pages (directory: \"page_1\", \"page_2\", etc) and a for loop on the html inside every page/directory (\"article_0, \"article_1\" and so on in \"page_1\" directory etc)\n",
    "for page in range(1,12): #for page in (1:301):\n",
    "    for file in range(0,100):\n",
    "        try:\n",
    "            # For every tsv file, I get the \"Plot\"\n",
    "            tsv_read = pd.read_csv('page_'+str(page)+'/article'+str((page-1)*100 + file)+'.tsv', sep='\\t')\n",
    "            PLOT = tsv_read['Plot'][0]\n",
    "            # I apply the stemming to the words in the Plot.\n",
    "            frase = porter.stem(PLOT)\n",
    "            frase_1 = frase\n",
    "            # I tokenize the word in the \"Plot\" and I consider every english stopwords. With a for loop on the words extract with tokenizer, I check if the word isn't a stopword. If so, I add this word to the list \"filtered_sentence\".\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            word_tokens = word_tokenize(frase_1)\n",
    "            filtered_sentence = [] \n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:  \n",
    "                    filtered_sentence.append(w)\n",
    "            filtered_sentence_without_punct = [] \n",
    "            # I create a \"list_without_punct\" with the words of the \"Plot\" without punctuation.\n",
    "            list_without_punct = tokenizer.tokenize(frase)\n",
    "            # With a for loop, I check if the words in \"filtered_sentence\" are also in \"list_without_punct\" (to discard punctuation). If so I add the word to filtered_sentence_without_punct.\n",
    "            for t in filtered_sentence:\n",
    "                if t in list_without_punct:\n",
    "                    filtered_sentence_without_punct.append(t)\n",
    "                    # I verify if a word is already present in the \"vocabulary\". If not, I add this word to the \"vocabulary\", associating it to a sequential integer (key). I also add that document to the \"inverted_index\" for that sequential integer (key).\n",
    "                    if t not in vocabulary.values():\n",
    "                        vocabulary[index_vocabulary] = t\n",
    "                        # Here I verify if the key is not present in the \"inverted_index\" to decide where insert the tab \n",
    "                        if index_vocabulary not in inverted_index.keys():\n",
    "                            inverted_index[index_vocabulary] ='article'+str((page-1)*100 + file)\n",
    "                        else:\n",
    "                            inverted_index[index_vocabulary] =',article'+str((page-1)*100 + file)\n",
    "                        index_vocabulary += 1\n",
    "                    # If that document is not yet present in the \"inverted_index\" for that sequential integer (key), I add it.\n",
    "                    elif ('article'+str((page-1)*100 + file) or ',article'+str((page-1)*100 + file))  not in inverted_index[list(vocabulary.keys())[list(vocabulary.values()).index(t)]]:\n",
    "                        inverted_index[list(vocabulary.keys())[list(vocabulary.values()).index(t)]] += ',article'+str((page-1)*100 + file)\n",
    "        except:\n",
    "            continue\n",
    "# I write the two dictionaries \"vocabulary\" and \"inverted_index\" on the relative json file.\n",
    "with open('vocabulary.json', 'w') as fp:\n",
    "    json.dump(vocabulary, fp)\n",
    "with open('inverted_index.json', 'w') as fp:\n",
    "    json.dump(inverted_index, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a query in input, the search engine returns information about documents that contain all the words in the query\n",
    "## Similarly to what was done previously for the words in the \"Plot\" of every book, I remove stopwords, punctuation and apply stemming from the query search.\n",
    "## I check if the words in the search query are registred in the 'vocabulary' and I get the relative key(term_id). I add these id to a list (\"list_of_term_id_input\").\n",
    "## From the term_id in \"list_of_term_id_input\", I extract the relative value (the article<i'> that contain that word) from the dict \"inverted_index\". I add this article<i'> to a list (of lists) \"list_of_term_id_input_in_dict\". That list contain the lists of article for every word.\n",
    "## I split the item in the list (of lists) \"list_of_term_id_input_in_dict\", getting every article<i'> and then I get an unique flat list from the list of lists. Now, if an article is present more times, it means that more of the input words are present in that article.\n",
    "## With Counter, I check the number of times an article<i'> appears in the flat list. If this number is equal to the number of the words entered in input, this means that all the input words are present in that article, so I have to exhibit it in output. To do this, I add this article to the list \"list_of_article\", then with a for cicle I extract all the articles from the various directories and I add them to a dataframe. I print that dataframe and also generate a tsv output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search: magic\n",
      "                  bookTitle  \\\n",
      "0  The Chronicles of Narnia   \n",
      "1         A Game of Thrones   \n",
      "2           Vampire Academy   \n",
      "\n",
      "                                                Plot  \\\n",
      "0  Journeys to the end of the world, fantastic cr...   \n",
      "1  Here is the first volume in George R. R. Marti...   \n",
      "2  ONLY A TRUE BEST FRIEND CAN PROTECT YOU FROM Y...   \n",
      "\n",
      "                                                 Url  \n",
      "0  https://www.goodreads.com/book/show/11127.The_...  \n",
      "1  https://www.goodreads.com/book/show/13496.A_Ga...  \n",
      "2  https://www.goodreads.com/book/show/345627.Vam...  \n"
     ]
    }
   ],
   "source": [
    "# Query and output\n",
    "query = input(\"Enter your search: \")\n",
    "# Similarly to what was done previously for the words in the \"Plot\" of every book, I remove stopwords, punctuation and apply stemming from the query search.\n",
    "frase = porter.stem(query)\n",
    "example_sent = frase\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [] \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:  \n",
    "        filtered_sentence.append(w)\n",
    "filtered_sentence_without_punct = [] \n",
    "list_without_punct = tokenizer.tokenize(frase)\n",
    "for t in filtered_sentence:\n",
    "    if t in list_without_punct:\n",
    "        filtered_sentence_without_punct.append(t)\n",
    "#I check if the words in the search query are registred in the 'vocabulary' and I get the relative key(term_id). I add these id to a list (\"list_of_term_id_input\").\n",
    "list_of_term_id_input = []\n",
    "for inp in filtered_sentence_without_punct:\n",
    "    if inp in vocabulary.values():\n",
    "        key = list(vocabulary.keys())[list(vocabulary.values()).index(inp)]\n",
    "        list_of_term_id_input.append(key)\n",
    "# From the term_id in \"list_of_term_id_input\", I extract the relative value (the article<i'> that contain that word) from the dict \"inverted_index\". I add this article<i'> to a list (of lists) \"list_of_term_id_input_in_dict\". That list contain the lists of article for every word.\n",
    "list_of_term_id_input_in_dict = []\n",
    "for term in list_of_term_id_input:\n",
    "    list_of_term_id_input_in_dict.append(inverted_index[term])\n",
    "# I split the item in the list (of lists) \"list_of_term_id_input_in_dict\", getting every article<i'> and then I get an unique flat list from the list of lists. Now, if an article is present more times, it means that more of the input words are present in that article.\n",
    "list_of_term_id_str = []\n",
    "for cont in list_of_term_id_input_in_dict:\n",
    "    list_of_term_id_str.append(cont.split(\",\"))\n",
    "# From the list of lists, I get an unique flat_list\n",
    "flat_list = []\n",
    "for sublist in list_of_term_id_str:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "# With Counter, I check the number of times an article<i'> appears in the flat list. If this number is equal to the number of the words entered in input, this means that all the input words are present in that article, so I have to exhibit it in output. To do this, I add this article to the list \"list_of_article\", then with a for cicle I extract all the articles from the various directories and I add them to a dataframe. I print that dataframe and also generate a tsv output.       \n",
    "conteggio = Counter(flat_list)\n",
    "list_of_article = []\n",
    "for c in conteggio:\n",
    "    if conteggio[c] == len(filtered_sentence_without_punct):\n",
    "        list_of_article.append(c)\n",
    "#list_of_article\n",
    "Output = pd.DataFrame(columns=['bookTitle','Plot','Url'])\n",
    "for page in range(1,12): #for page in (1:301):\n",
    "    try:\n",
    "        for article in list_of_article:\n",
    "            tsv_read = pd.read_csv('page_'+str(page)+'/'+str(article)+'.tsv', sep='\\t')\n",
    "            row = [tsv_read['bookTitle'][0], tsv_read['Plot'][0], tsv_read['Url'][0]]\n",
    "            Output.loc[len(Output)] = row\n",
    "    except:\n",
    "        continue\n",
    "print(Output)\n",
    "Output.to_csv('Output.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
